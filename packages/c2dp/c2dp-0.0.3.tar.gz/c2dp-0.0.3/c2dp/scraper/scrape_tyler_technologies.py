# -*- coding: utf-8 -*-
"""
Generalized Scraper for Tyler Technology Websites (also Odyssey) 

Created on Thu Aug  8 11:12:46 2019

@author: bernsteind
"""


from bs4 import BeautifulSoup as bs
import os, sys, re
import pandas as pd
import numpy as np
from itertools import product
from string import ascii_lowercase, digits
import datetime
import logging
import itertools
import boto3
from requestium import Session, Keys, Select
from selenium.webdriver.support.ui import Select
import simplejson as json




def setup_dirs(query_type):
    if not os.path.exists(query_type + '/case_info'):
        os.makedirs(query_type + '/case_info')
    if not os.path.exists(query_type + '/logs'):
        os.makedirs(query_type + '/logs')


def setup_driver(driver_path='C:/Users/bernsteind/Documents/chromedriver/chromedriver',
                 default_timeout=30, 
                 browser='chrome',
                 headless=False
                 ):
    """
    set up the webdriver that will 

    Params
    ------
    driver_path : str
        the path to the web driver execution file
    default_timeout : str
        the request timeout maximum
    browser : str
        the browser to use for selenium navigation (default: Chrome)
    headless : bool
        whether to operate in headless mode or not (headless means no window will be presented to the user)
    Returns
    -------
    Driver session
        Returns a Requestium driver session that has properties of a request session (sess) and a selenium driver (sess.driver)
    """
  #  prefs = {"download.default_directory" : query_type + "/case_info"}
  #  chromeOptions.add_experimental_option("prefs",prefs)

    if headless:
        s = Session(webdriver_path=driver_path,
                    browser=browser,
                    default_timeout=default_timeout,
                    webdriver_options={'arguments' : ['headless']})
                
    else:
        s = Session(webdriver_path=driver_path,
                    browser=browser,
                    default_timeout=default_timeout
                    )

    return s



def get_location_options(sess, 
                         url,
                         suffix=''):
    """
    navigate to the locale court search landing page and extract all 'location' options

    Params
    ------
    url : str
        the locale search landing page
    suffix : tuple (optional)
        a tuple of suffix strings to search for in the options
    Returns
    -------
    list of tuples
        Returns a list of tuples with the tuple structure: ('text of option', 'css value attribute of option')
    """
 #   sess=setup_driver(headless=True)

    sess.driver.get(url)
    select = Select(sess.driver.ensure_element_by_id("sbxControlID2"))

    geo_values = []
    geo_names = []

    if suffix:
        for i in select.options: 
            if i.text.endswith((suffix)):
                geo_values.append(i.get_attribute("value"))
                geo_names.append(i.text)
    else:
        for i in select.options: 
            geo_values.append(i.get_attribute("value"))
            geo_names.append(i.text)
            
    geo_zip = list(zip(geo_names, geo_values))
    
    return geo_zip


def pass_page(sess, 
              url,
              select_text,
              link_text,
              select_type = "visible text"):
    """
    navigate to and pass the locale court search landing page, including selecting a 'location' and a case type link 

    Params
    ------
    sess : object
        the Requestium sess object generated by `setup_driver`
    url : str
        the locale search landing page
    select_text : str
        the option to select from the "select a location" dropdown
    link_text : str
        the link to click to proceed
    select_type : str
        the method for selecting `select_text`, can be either 'visible text' (to use the HTML visible text) or 'value' (to use css attribute `value`)
    Returns
    -------
    Driver session
        Returns a Requestium driver session that has navigated past the landing page
    """
    sess.driver.get(url)
    # get past the first page, selecting the whole geography
    print('started getting past first page')
    select = Select(sess.driver.ensure_element_by_id("sbxControlID2"))
    if select_type == "visible text":
        select.select_by_visible_text(select_text)
    elif select_type == "value":
        select.select_by_value(select_text)
    link = sess.driver.ensure_element_by_link_text(link_text)

    t0=datetime.datetime.now()
    link.ensure_click()
    duration=datetime.datetime.now()-t0

    print('finished getting past first page')
    return duration


def set_searchBy(sess,
                 option_text='Date Filed'):
    searchBy_type = sess.driver.ensure_element_by_name("SearchBy").get_attribute('type')
    if searchBy_type == 'select-one':
        el = Select(sess.driver.ensure_element_by_id("SearchBy"))
        el.select_by_visible_text(option_text).click()
    elif searchBy_type == 'radio':
        sess.driver.ensure_element_by_css_selector("input[type='radio'][labelvalue='{0}']".format(option_text)).ensure_click()


def get_caseTypes(sess,
                  suffix=''):
    select = Select(sess.driver.ensure_element_by_id("selCaseTypeGroups"))

    case_values = []
    case_names = []

    if suffix:
        for i in select.options: 
            if i.text.endswith((suffix)):
                case_values.append(i.get_attribute("value"))
                case_names.append(i.text)
    else:
        for i in select.options: 
            case_values.append(i.get_attribute("value"))
            case_names.append(i.text)
            
    type_zip = list(zip(case_names, case_values))
    
    return type_zip


def set_caseType(sess,
                 option_text):
    dropdown_options = sess.driver.ensure_element_by_id("selCaseTypeGroups")
    dropdown_options.send_keys(option_text)

def set_dates(sess,
              date_start, date_end):
    sess.driver.ensure_element_by_id('DateFiledOnAfter').send_keys(date_start)
    sess.driver.ensure_element_by_id('DateFiledOnBefore').send_keys(date_end)

def set_last_names(sess, last_name):
    if last_name:
        sess.driver.ensure_element_by_id('LastName').send_keys(last_name)

def set_first_names(sess, first_name):
    if first_name:
        sess.driver.ensure_element_by_id('FirstName').send_keys(first_name)

def set_case_number(sess, case_number):
    if case_number:
        sess.driver.ensure_element_by_id('CaseSearchValue').send_keys(case_number)


def submit_form(sess):
    t0=datetime.datetime.now()
    sess.driver.ensure_element_by_id('SearchSubmit').ensure_click()
    duration=datetime.datetime.now()-t0
    return duration

def generate_queryDict(sess):
    searchParams=sess.driver.ensure_element_by_xpath('//*[@id="SearchParamList"]')
    string_list=searchParams.text.split('   ')
    query_dict = {}
    included_strings = ["Date Filed On or After: ","Date Filed On or Before: ", 
                        "Last Name: ", "First Name: ",
                        "Case Number: ", "Case Types: "]
    for i, j in list(itertools.product(string_list, included_strings)):
        if j in i:
            var = i.split(': ', 1)[0]
            val = i.split(': ', 1)[1]
            query_dict[var] = val

    return query_dict


def get_recordCount(sess, 
                    query_type,
                    query_dict,
                    query_vars=["Date Filed On or After","Date Filed On or Before"],
                    other_vars ='',
                    write=False,
                    limit=''):
    query_list=[]
    for x, y in list(itertools.product(query_dict, query_vars)):
        if y in x:
            query_list.append(query_dict[x].replace(',', '-'))

    if other_vars:
        queryString = '_'.join(query_list + [other_vars])
    else:
        queryString = '_'.join(query_list)

        
    # get the number of results
    numResults = bs(sess.driver.page_source).select('table:nth-child(4) tr:nth-child(1) td:nth-child(2) b')[0].text
    # log number of results 
    if write:
        output_file = query_type + '/logs/record_count.txt'
        with open(output_file, "a") as file:
            file.write('{0},{1}\n'.format(queryString, numResults))
    if limit:
        if int(numResults) >= int(limit):
            output_file = query_type + '/logs/record_count_limit.txt'
            with open(output_file, "a") as file:
                file.write('{0},{1}\n'.format(queryString, numResults))

    return numResults


def get_resultsTable(sess, 
                     query_type,
                     query_dict, 
                     record_count,
                     file_vars=["Date Filed On or After"],
                     write=True):

    file_list=[]            
    for x, y in list(itertools.product(query_dict, file_vars)):
        if y in x:
            file_list.append(query_dict[x])

    fileString = '_'.join(file_list).replace('/', '-')

    soup = bs(sess.driver.page_source)
    caseUrls = soup.find_all('a', {'href': re.compile(r'CaseDetail')})
                    
    case_href = []
    case_num = []
    for x in caseUrls:
        case_href.append(x['href'])
        case_num.append(x.text.strip())
                
    case_id_df = pd.DataFrame(zip(case_href, case_num), columns = ['case_href', 'Case Number'])
                    
    result_table = pd.read_html(str(soup(text = "Case Number")[0].find_parent('table')))[0]
    result_table = result_table.astype(str)
    result_table['Case Number'] = result_table['Case Number'].str.replace('[.]0$', '', regex=True).str.strip()
    if 'Citation Number' in result_table.columns:
        result_table['Citation Number'] = result_table['Citation Number'].str.replace('[.]0$', '', regex=True)
    result_table = result_table.dropna(how='all')  
    result_table['Case Number'] = result_table['Case Number'].astype(str)
    result_table = pd.merge(result_table, case_id_df, on='Case Number')
    
    if write:
        output_file = query_type + "/case_info/{0}.txt".format(fileString)
        result_table.to_csv(output_file, mode = "a", header=False,index=False)
            
    return result_table

def get_HTML(sess, 
             base_url, 
             resultTable, 
             location, 
             date, 
             client, 
             sm,
             ext="htmls_civil_cases"):
    sess.transfer_driver_cookies_to_session()
    case_href = resultTable.iloc[:,-1]
    link = 0     

    # scrape each case HTML file listed on the page
    while link < len(case_href):
        try:         
            case_url = base_url.rsplit('/', 1)[0] + '/' + case_href[link]
            date_list = str(date).split(' ')[0].split('/')
            date_ext = "{0}/{1}/{2}".format(date_list[2], date_list[0], date_list[1])
            
            
            res_case_html = sess.get(case_url)
           # sm.track_requests(res_case_html)
          #  res_case_html.content
            delay = sm.track_request(res_case_html, "scrape_html")
            client.put_object(Body=res_case_html.content, Bucket='odga', 
                              Key='{0}/raw/{1}/{2}/{3}.html'.format(location, ext, date_ext, case_href[link].replace("\\`*{}[]()>#+.!$", "_"))
                                   )
                                   
            link += 1
        except Exception:
            print('failed to gather individual case HTML')
            logging.exception("error in get_HTML loop: {0}".format('_'.join([str(date), case_href[link]])), exc_info=True)
            continue
        
        

def run_loop(searchBy,
             select_text, 
             link_text, 
             select_type, 
             sess_input, 
             sm, 
             base_url, 
             df, 
             row_index, 
             query_type, 
             query_vars, 
             query_limit,
             file_vars,
             caseTypes=True,
             name=''):
    
    
    sess = setup_driver(headless=True)
    opts = get_location_options(base_url)
    
    response = pass_page(sess, 
                         url=base_url,
                         select_text=select_text,
                         link_text=link_text,
                         select_type = select_type)

    sm.track_request(response, 'pass_page')
# set up the loop dataframe
    while row_index < df.shape[0]:    
        try:
            row = df.iloc[row_index]
            # setup the directories for the scraping
            
            caseTypes = get_caseTypes(sess)
            
            set_searchBy(sess, searchBy)
            set_dates(sess, row['date'], row['date'])
            
            if 'FirstName' in row.index:
                set_first_names(sess, row['FirstName'])

            if 'LastName' in row.index:
                set_last_names(sess, row['LastName'])

            
            duration = submit_form(sess)
            sm.track_request(duration, 'submit_form')
            #print(sm.responses)
            queryDict = generate_queryDict(sess)
            record_count = get_recordCount(sess, 
                                           query_type = query_type,
                                           query_dict=queryDict,
                                           query_vars=query_vars,
                                           write=True, 
                                           limit=query_limit)
            
            # if the record count matches the query limit, we go back and scrape for each case type
            if (record_count == str(query_limit)) and (caseTypes):
                sess.driver.back()
            
                case_type_index = 0
                while case_type_index < len(caseTypes):
                    set_searchBy(sess, 'Date Filed')
                    set_dates(sess, row['date'], row['date'])
                    set_caseType(sess, caseTypes[case_type_index][0])
                            
                    duration = submit_form(sess)
                    sm.track_request(duration, 'submit_form')
                    #print(sm.responses)
                    queryDict = generate_queryDict(sess)
                    record_count = get_recordCount(sess, 
                                                   query_type = query_type,
                                                   query_dict=queryDict,
                                                   query_vars=["Date Filed On or After","Date Filed On or Before"],
                                                   write=True, 
                                                   limit=query_limit)
                   # print(record_count)
                    sess.driver.back()
                    case_type_index+=1
            
            if record_count != '0':
                resultTable = get_resultsTable(sess,
                                               query_type=query_type,
                                               query_dict=queryDict, 
                                               record_count=record_count,
                                               file_vars=file_vars,
                                               write=True)
                
    #            get_HTML(sess, base_url, resultTable, location, row['date'], client, ext="htmls_civil_cases")
                
            # return to query page
            sess.driver.back()
            row_index+=1
        except:
            print('failed')
            logging.exception("error in query loop: {0}".format(str(row_index)), exc_info=True)
            sess.driver.quit()
            del(sess)
            sess = setup_driver(headless=True)
            response = pass_page(sess, 
                         url=base_url,
                         select_text=select_text,
                         link_text=link_text,
                         select_type = select_type)
            sm.track_request(response, 'pass_page')
            
            
def tidy_split(df, column, sep='|', keep=False):
    """
    Split the values of a column and expand so the new DataFrame has one split
    value per row. Filters rows where the column is missing.

    Params
    ------
    df : pandas.DataFrame
        dataframe with the column to split and expand
    column : str
        the column to split and expand
    sep : str
        the string used to split the column's values
    keep : bool
        whether to retain the presplit value as it's own row

    Returns
    -------
    pandas.DataFrame
        Returns a dataframe with the same columns as `df`.
    """
    indexes = list()
    new_values = list()
    df = df.dropna(subset=[column])
    for i, presplit in enumerate(df[column].astype(str)):
        values = presplit.split(sep)
        if keep and len(values) > 1:
            indexes.append(i)
            new_values.append(presplit)
        for value in values:
            indexes.append(i)
            new_values.append(value)
    new_df = df.iloc[indexes, :].copy()
    new_df[column] = new_values
    return new_df

