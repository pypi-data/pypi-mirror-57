# gpt-2-finetuning

This is a package for finetuning GPT-2 models.

It is based on the work done by:

- [OpenAI's official GPT-2 repository](https://github.com/openai/gpt-2)
- Finetuning [functionality from nshepperd's fork](https://github.com/nshepperd/gpt-2) of the official GPT-2 repository

## Usage

Install the package

```sh
pip install gpt_2_finetuning
```

Download GPT-2 models through terminal

```sh
# Available model sizes: 124M, 355M, 774M
# Example command to download model
download_gpt2_model 124M
```

Example usage:

[Generating samples from GPT-2 model](https://www.kaggle.com/jonhengcp/gpt2-774m-encoding-generating-samples)

[Finetuning GPT-2 model with Shakespeare's works](https://www.kaggle.com/jonhengcp/gpt2-finetuning)

## Further reading

Code from the paper ["Language Models are Unsupervised Multitask Learners"](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf).

See more details in our [blog post](https://blog.openai.com/better-language-models/).

## GPT-2 samples

| WARNING: Samples are unfiltered and may contain offensive content. |
| ------------------------------------------------------------------ |


## License

[MIT](./LICENSE)
