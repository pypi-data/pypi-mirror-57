{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SV<sup>2</sup> Classifier Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Getting Started](#readme)\n",
    "\n",
    "2. [Input](#input)\n",
    "  * Get Your Training Data\n",
    "     * [SV<sup>2</sup> Training Data](#sv2default)\n",
    "     * [Generate New Training Data](#sv2train)\n",
    "  * [Copy Number Input](#copynumber)\n",
    "\n",
    "3. [Name Your Classifer](#clf)\n",
    "\n",
    "4. Training\n",
    "  * [Deletion > 1000 Classifier](#delgt)\n",
    "\n",
    "  * [Deletion <= 1000 Classifier](#dellt)\n",
    "\n",
    "  * [Deletion Male Sex Chromosomes Classifier](#delmsc)\n",
    "\n",
    "  * [Duplication Breakpoint Classifier](#dupbrk)\n",
    "\n",
    "  * [Duplication SNV Classifier](#dupsnv)\n",
    "\n",
    "  * [Duplication Male Sex Chromosomes Classifier](#dupmsc)\n",
    "\n",
    "5. [Loading New Classifiers](#dump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {margin-left: 0 !important;}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {margin-left: 0 !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This jupyter notebook is a guide for training the SVM genotyping classifers for SV<sup>2</sup>. <a class=\"anchor\" id=\"readme\"></a>\n",
    "\n",
    "This notebook is currently formatted to train the default classifiers that SV<sup>2</sup> **versions >=1.2** uses.\n",
    "\n",
    "The parameters for the classifier are the ones used for the default classifiers. To retrain the models, alter these values.\n",
    "\n",
    "Feel free to edit this document to train your own data. \n",
    "  * Classifiers will be saved as pickle (.pkl) files. \n",
    "  * Paths to classifier pickle files are saved as a JSON file.\n",
    "\n",
    "#### For questions please contact Danny Antaki: dantaki@ucsd.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input <a class=\"anchor\" id=\"input\"></a>\n",
    "\n",
    "Get your training data\n",
    "\n",
    "### 1. Train using the default training set <a class=\"anchor\" id=\"sv2default\"></a>\n",
    "\n",
    "The default training set is located here\n",
    "\n",
    "```\n",
    "$ ls SV2-INSTALL-PATH/sv2/training/1kgp_training_data/*\n",
    "\n",
    "    1kgp_highcov_del_gt1kb.txt  \n",
    "    1kgp_highcov_del_lt1kb.txt  \n",
    "    1kgp_highcov_del_malesexchrom.txt  \n",
    "    1kgp_highcov_dup_snv.txt  \n",
    "    1kgp_lowcov_dup_breakpoint.txt  \n",
    "    1kgp_lowcov_dup_malesexchrom.txt\n",
    "\n",
    "```\n",
    "\n",
    "### 2. Generate training input with `sv2train` <a class=\"anchor\" id=\"sv2train\"></a>\n",
    "\n",
    "```\n",
    "sv2train -i <in.bam> -snv <snv.vcf.gz ...> -b <sv.bed ...> -v <sv.vcf ...> -p <in.ped ...> \n",
    "```\n",
    "\n",
    "Training input files located here\n",
    "```\n",
    "$ ls sv2_training_features/*\n",
    "\n",
    "    sv2_training_features_deletion_gt1kb.txt\n",
    "    sv2_training_features_deletion_lt1kb.txt\n",
    "    sv2_training_features_deletion_male_sex_chrom.txt\n",
    "    sv2_training_features_duplication_breakpoint.txt\n",
    "    sv2_training_features_duplication_male_sex_chrom.txt\n",
    "    sv2_training_features_duplication_snv.txt\n",
    "\n",
    "$ head -n 2 sv2_training_features/sv2_deletion_gt1kb_training_features.txt\n",
    "```\n",
    "\n",
    "\n",
    "| chr | start | end | type |\tid  | covr | dpe_rat | sr_rat | ... | copy_number | classifier |\n",
    "| :--- | ----- | --- | ---- |  --- | ---- | ------- | ------ | --- | ----------- | ---------- |\n",
    "| chr1 | 193646553 | 193654283 | DEL  |\tHG00096\t| 1.145 | 0.0 |\t0.0 | ... | **NA** |deletion_gt1kb |\n",
    " \n",
    "This jupyter notebook will accept training data in a pandas DataFrame given the following column names\n",
    "\n",
    "| column | description | \n",
    "| ------ | ----------- |\n",
    "| covr   | depth of coverage  feature |\n",
    "| dpe_rat | discordant paired-end feature |\n",
    "| sr_rat  | split-read feature | \n",
    "| HET_ratio | heterozygous allele depth feature | \n",
    "| copy_number | [copy number](#copynumber) |\n",
    "\n",
    "### Copy Number Input <a class=\"anchor\" id=\"copynumber\"></a> \n",
    "\n",
    "sv2train does not produce genotypes, hence copy_number values are not available (**NA**). The user will have to supply genotypes for training custom data sets. \n",
    "\n",
    "The included default SV<sup>2</sup> training sets contain copy_number values\n",
    "\n",
    "This jupyter notebook expects the following copy_number values\n",
    "\n",
    "#### Biallelic SVs\n",
    "\n",
    "| copy_number | VCF genotype |\n",
    "| :----------- | ------------ |\n",
    "| 4           | 1/1 (HOM)    |\n",
    "| 3           | 0/1 (HET)    | \n",
    "| 2           | 0/0  (REF)   |\n",
    "| 1           | 0/1  (HET)   |\n",
    "| 0           | 1/1  (HOM)   |\n",
    "\n",
    "#### SVs on Male Sex Chromosomes\n",
    "\n",
    "| copy_number | VCF genotype |\n",
    "| :----------- | ------------ |\n",
    "| 2           | 1  (ALT)   | \n",
    "| 1           | 0  (REF)   |\n",
    "| 0           | 1  (ALT)   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOUR_CLF <a class=\"anchor\" id=\"clf\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "YOUR_CLF=\"default\" # for custom classifiers please change this name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "         WARNING:\n",
    "DO NOT ALTER THE KEY NAMES\n",
    "\"\"\"\n",
    "clf_json={YOUR_CLF : \n",
    "          {\n",
    "              'delgt': None,   #deletion >1000bp clf\n",
    "              'dellt' : None,  #deletion <= 1000bp clf\n",
    "              'delmsc' : None, #deletion male sex chroms clf\n",
    "              'dupbrk' : None, #duplication breakpoint clf\n",
    "              'dupsnv' : None, #duplication snv clf\n",
    "              'dupmsc' : None  #duplication male sex chroms clf\n",
    "          }\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from os import getcwd\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "try: import cPickle as pickle\n",
    "except ImportError: import pickle\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# global functions\n",
    "def EuclidDist(a,x,b,y,coef): return 1 / sqrt(( ((a-x)**2) + ((b-y)**2) + coef ) )\n",
    "def openTrain(path): \n",
    "    df=pd.read_csv(path,sep=\"\\t\")\n",
    "    return df[df['covr']< 5.0] # do not train on outliers\n",
    "def dup_convert_msc(df):\n",
    "    cn=[]\n",
    "    for x in df['copy_number']:\n",
    "        gt=1\n",
    "        if x>1: gt=0\n",
    "        cn.append(gt)\n",
    "    df['copy_number']=cn\n",
    "    return df\n",
    "def dup_convert(df):\n",
    "    # convert duplication copy numbers\n",
    "    ## 3 -> 1 : HET\n",
    "    ## 4 -> 0 : HOM\n",
    "    cn=[]\n",
    "    for x in df['copy_number']:\n",
    "        gt=2\n",
    "        if x==3: gt=1\n",
    "        if x==4: gt=0\n",
    "        cn.append(gt)\n",
    "    df['copy_number']=cn\n",
    "    return df\n",
    "def biallelic_weight(df,gts,hetexp,homexp,coef=0.01):\n",
    "        # generate weights for biallelic SVs\n",
    "        ## genotypes (gts) must be ordered as [REF, HET, HOM]\n",
    "        ## hetexp, homexp are expected HET and HOM normalized coverages\n",
    "        ## coef to avoid dividing by zero\n",
    "        final=pd.DataFrame()\n",
    "        for gtype in gts:\n",
    "                cn = df[df['copy_number'] == gtype]\n",
    "                covr = cn['covr']\n",
    "                cntr = 1.0\n",
    "                if gtype == gts[1]: cntr = hetexp\n",
    "                elif gtype == gts[2]: cntr = homexp\n",
    "                weights = [ 1.0/(abs(cntr-x)+coef) for x in covr] #inverse distance weights\n",
    "                cn['weights']=weights\n",
    "                final = final.append(cn)\n",
    "        return final\n",
    "def allele_weight(df,gts,altexp,coef=0.01):\n",
    "        # generate weights for SVs on male sex chroms\n",
    "        ## genotypes (gts) must be ordered as [REF,ALT]\n",
    "        ## altexp is the ALT expected normalized coverages\n",
    "        ## coef to avoid dividing by zero\n",
    "        final=pd.DataFrame()\n",
    "        for gtype in gts:\n",
    "                cn = df[df['copy_number'] == gtype]\n",
    "                covr = cn['covr']\n",
    "                cntr = 1.0\n",
    "                if gtype == gts[1]: cntr = altexp\n",
    "                weights = [ 1.0/(abs(cntr-x)+coef) for x in covr] #inverse distance weights\n",
    "                cn['weights']=weights\n",
    "                final = final.append(cn)\n",
    "        return final\n",
    "def SNV_weight(df,gts,hetexp,homexp,coef=0.01):\n",
    "        # generate weights for SVs with heterozygous allele depth (HAD) features\n",
    "        ## genotypes (gts) must be ordered as [REF,HET,HOM]\n",
    "        ## hetexp, homexp are expected HET and HOM normalized coverages\n",
    "        ## coef to avoid dividing by zero\n",
    "        final=pd.DataFrame()\n",
    "        cn2med = np.median(df[df['copy_number']==gts[0]]['HET_ratio']) # median HAD ratio for REF\n",
    "        cn3med = np.median(df[df['copy_number']==gts[1]]['HET_ratio']) # median HAD ratio for HET\n",
    "        cn4med = cn2med/2.0 # median HAD ratio for HOM 2.0 is a heuristic based on density estimates  \n",
    "        for gtype in gts:\n",
    "                wgts=[]\n",
    "                cn = df[df['copy_number'] == gtype]\n",
    "                covr = cn['covr']\n",
    "                het = cn2med\n",
    "                cntr = 1.0\n",
    "                if gtype == gts[1]: het,cntr = cn3med,hetexp\n",
    "                elif gtype == gts[2]: het,cntr=cn2med,homexp\n",
    "                for ind,ele in cn.iterrows():\n",
    "                        wgt = EuclidDist(cntr,ele['covr'],het,ele['HET_ratio'],coef) # inverse Euclidean distance weights\n",
    "                        wgt2 = None\n",
    "                        if gtype == gts[2]:\n",
    "                                # HOM weight for AAAa\n",
    "                                wgt2 = EuclidDist(cntr,ele['covr'],cn4med,ele['HET_ratio'],coef)\n",
    "                        if wgt2 != None: \n",
    "                            # maximum weight for AAaa vs. AAAa\n",
    "                            if wgt2 > wgt: wgt=wgt2\n",
    "                        wgts.append(wgt)\n",
    "                cn['weights']=wgts\n",
    "                final = final.append(cn)\n",
    "        return final\n",
    "def transpose_features(df): return np.vstack(np.asarray( (df['covr'],df['dpe_rat'],df['sr_rat']), order = 'C', dtype='float' )).T\n",
    "def transpose_snv(df): return np.vstack(np.asarray( (df['covr'],df['HET_ratio']), order = 'C', dtype='float' )).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deletion > 1000bp Classifer <a class=\"anchor\" id=\"delgt\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CLF_NAME = \"1kgp_highcov_del_gt1kb_svm_clf.pkl\"\n",
    "df = openTrain(\"1kgp_training_data/1kgp_highcov_del_gt1kb.txt\")\n",
    "# generate weights\n",
    "df = biallelic_weight(df,[2,1,0],0.5,0.0)\n",
    "# prepare data structures\n",
    "x,y,wgt = transpose_features(df), np.array(df['copy_number']), np.array(df['weights'])\n",
    "# train the SVM\n",
    "#### RBF KERNEL PARAMETERS ####\n",
    "C = 0.01 \n",
    "GAMMA = 10\n",
    "###############################\n",
    "MEM = 12000 # in MB\n",
    "clf = svm.SVC(kernel='rbf', \n",
    "                     probability=True,\n",
    "                     random_state=42,\n",
    "                     C=C,\n",
    "                     gamma=GAMMA,\n",
    "                     class_weight='balanced',\n",
    "                     cache_size=MEM)\n",
    "# this might take awhile!\n",
    "clf.fit(x,y,sample_weight=wgt)\n",
    "# svm model persistance\n",
    "pickle.dump(clf, open(CLF_NAME, 'wb'))\n",
    "clf_json[YOUR_CLF]['delgt']=getcwd()+'/'+CLF_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deletion <= 1000 Classifier <a class=\"anchor\" id=\"dellt\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CLF_NAME = \"1kgp_highcov_del_lt1kb_svm_clf.pkl\"\n",
    "df = openTrain(\"1kgp_training_data/1kgp_highcov_del_lt1kb.txt\")\n",
    "# generate weights\n",
    "df = biallelic_weight(df,[2,1,0],0.5,0.0)\n",
    "# prepare data structures\n",
    "x,y,wgt = transpose_features(df), np.array(df['copy_number']), np.array(df['weights'])\n",
    "# train the SVM\n",
    "#### RBF KERNEL PARAMETERS ####\n",
    "C = 1000\n",
    "GAMMA = 1\n",
    "###############################\n",
    "MEM = 12000 # in MB\n",
    "clf = svm.SVC(kernel='rbf', \n",
    "                     probability=True,\n",
    "                     random_state=42,\n",
    "                     C=C,\n",
    "                     gamma=GAMMA,\n",
    "                     class_weight='balanced',\n",
    "                     cache_size=MEM)\n",
    "# this might take awhile!\n",
    "clf.fit(x,y,sample_weight=wgt)\n",
    "# svm model persistance\n",
    "pickle.dump(clf, open(CLF_NAME, 'wb'))\n",
    "clf_json[YOUR_CLF]['dellt']=getcwd()+'/'+CLF_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deletion Male Sex Chrom Classifer <a class=\"anchor\" id=\"delmsc\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CLF_NAME = \"1kgp_highcov_del_malesexchrom_svm_clf.pkl\"\n",
    "df = openTrain(\"1kgp_training_data/1kgp_highcov_del_malesexchrom.txt\")\n",
    "# generate weights\n",
    "df = allele_weight(df,[1,0],0.0)\n",
    "# prepare data structures\n",
    "x,y,wgt = transpose_features(df), np.array(df['copy_number']), np.array(df['weights'])\n",
    "# train the SVM\n",
    "#### RBF KERNEL PARAMETERS ####\n",
    "C = 1\n",
    "GAMMA = 1\n",
    "###############################\n",
    "MEM = 12000 # in MB\n",
    "clf = svm.SVC(kernel='rbf', \n",
    "                     probability=True,\n",
    "                     random_state=42,\n",
    "                     C=C,\n",
    "                     gamma=GAMMA,\n",
    "                     class_weight='balanced',\n",
    "                     cache_size=MEM)\n",
    "# this might take awhile!\n",
    "clf.fit(x,y,sample_weight=wgt)\n",
    "# svm model persistance\n",
    "pickle.dump(clf, open(CLF_NAME, 'wb'))\n",
    "clf_json[YOUR_CLF]['delmsc']=getcwd()+'/'+CLF_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplication Breakpoint Classifier <a class=\"anchor\" id=\"dupbrk\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CLF_NAME = \"1kgp_lowcov_dup_breakpoint_svm_clf.pkl\"\n",
    "df = dup_convert(openTrain(\"1kgp_training_data/1kgp_lowcov_dup_breakpoint.txt\"))\n",
    "# generate weights\n",
    "df = biallelic_weight(df,[2,1,0],1.5,2.0)\n",
    "# prepare data structures\n",
    "x,y,wgt = transpose_features(df), np.array(df['copy_number']), np.array(df['weights'])\n",
    "# define class weights NOTE: you may want to try balanced weights before doing this!\n",
    "class_wgt = { \n",
    "    0 : 2,\n",
    "    1 : 0.5,\n",
    "    2 : 1\n",
    "}\n",
    "# train the SVM\n",
    "#### RBF KERNEL PARAMETERS ####\n",
    "C = 1\n",
    "GAMMA = 10\n",
    "###############################\n",
    "MEM = 12000 # in MB\n",
    "clf = svm.SVC(kernel='rbf', \n",
    "                     probability=True,\n",
    "                     random_state=42,\n",
    "                     C=C,\n",
    "                     gamma=GAMMA,\n",
    "                     class_weight=class_wgt, # custom class weights\n",
    "                     cache_size=MEM)\n",
    "# this might take awhile!\n",
    "clf.fit(x,y,sample_weight=wgt)\n",
    "# svm model persistance\n",
    "pickle.dump(clf, open(CLF_NAME, 'wb'))\n",
    "clf_json[YOUR_CLF]['dupbrk']=getcwd()+'/'+CLF_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplication SNV Classifier <a class=\"anchor\" id=\"dupsnv\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CLF_NAME = \"1kgp_highcov_dup_snv_svm_clf.pkl\"\n",
    "df = dup_convert(openTrain(\"1kgp_training_data/1kgp_highcov_dup_snv.txt\"))\n",
    "# define some types (just in case)\n",
    "df[['HET_ratio']]=df[['HET_ratio']].astype(float)\n",
    "# generate weights\n",
    "df = SNV_weight(df,[2,1,0],1.5,2.0)\n",
    "# prepare data structures\n",
    "x,y,wgt = transpose_snv(df), np.array(df['copy_number']), np.array(df['weights'])\n",
    "# train the SVM\n",
    "#### RBF KERNEL PARAMETERS ####\n",
    "C = 0.01\n",
    "GAMMA = 5000\n",
    "###############################\n",
    "MEM = 12000 # in MB\n",
    "clf = svm.SVC(kernel='rbf', \n",
    "                     probability=True,\n",
    "                     random_state=42,\n",
    "                     C=C,\n",
    "                     gamma=GAMMA,\n",
    "                     class_weight='balanced',\n",
    "                     cache_size=MEM)\n",
    "# this might take awhile!\n",
    "clf.fit(x,y,sample_weight=wgt)\n",
    "# svm model persistance\n",
    "pickle.dump(clf, open(CLF_NAME, 'wb'))\n",
    "clf_json[YOUR_CLF]['dupsnv']=getcwd()+'/'+CLF_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplication Male Sex Chrom Classifer <a class=\"anchor\" id=\"dupmsc\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CLF_NAME = \"1kgp_lowcov_dup_malesexchrom_svm_clf.pkl\"\n",
    "df = dup_convert_msc(openTrain(\"1kgp_training_data/1kgp_lowcov_dup_malesexchrom.txt\")) \n",
    "# generate weights\n",
    "df = allele_weight(df,[1,0],2.0) # expected ALT in the default training set. You may want to change this for custom training\n",
    "# prepare data structures\n",
    "x,y,wgt = transpose_features(df), np.array(df['copy_number']), np.array(df['weights'])\n",
    "# define class weights NOTE: you may want to try balanced weights before doing this!\n",
    "class_wgt = { \n",
    "    0 : 1,\n",
    "    1 : 1\n",
    "}\n",
    "# train the SVM\n",
    "#### RBF KERNEL PARAMETERS ####\n",
    "C = 1000\n",
    "GAMMA = 0.01\n",
    "###############################\n",
    "MEM = 12000 # in MB\n",
    "clf = svm.SVC(kernel='rbf', \n",
    "                     probability=True,\n",
    "                     random_state=42,\n",
    "                     C=C,\n",
    "                     gamma=GAMMA,\n",
    "                     class_weight=class_wgt, # custom class weights\n",
    "                     cache_size=MEM)\n",
    "# this might take awhile!\n",
    "clf.fit(x,y,sample_weight=wgt)\n",
    "# svm model persistance\n",
    "pickle.dump(clf, open(CLF_NAME, 'wb'))\n",
    "clf_json[YOUR_CLF]['dupmsc']=getcwd()+'/'+CLF_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Classifiers to SV<sup>2</sup> <a class=\"anchor\" id=\"dump\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **!** Run the cell below to save your work **!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outjson = YOUR_CLF+\".json\"\n",
    "ofh = open(outjson,'w')\n",
    "json.dump(clf_json,ofh,indent=4)\n",
    "ofh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the following commands below\n",
    "```\n",
    "# add classifiers to SV2\n",
    "sv2 -load-clf <YOUR_CLF.json>\n",
    "\n",
    "# genotype with custom classifiers\n",
    "sv2 ... -clf <YOUR_CLF>\n",
    "\n",
    "# for example:\n",
    "$ sv2 -load-clf default.json\n",
    "$ sv2 -clf default\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contact: Danny Antakli dantaki@ucsd.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
