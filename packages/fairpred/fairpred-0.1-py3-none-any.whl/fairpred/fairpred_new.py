# -*- coding: utf-8 -*-
"""FairPred

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1unSJuvRr3WKVGwADz2BsWBy3u-iLmyK-
"""

# !pip install aif360

"""fair_pred
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1unSJuvRr3WKVGwADz2BsWBy3u-iLmyK-
"""

"""	FairPred FairPredict:
	Gets data and protected column and 
"""

import os
import pandas as pd
import numpy as np 
from typing import List
from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric
from aif360.algorithms.preprocessing.reweighing import Reweighing
from aif360.algorithms.preprocessing import DisparateImpactRemover
from aif360.algorithms.preprocessing.lfr import LFR
from collections import defaultdict
# from sklearn.linear_model import LogisticRegression
from sklearn import tree
from aif360.algorithms import Transformer
from aif360.algorithms.preprocessing.lfr_helpers import helpers as lfr_helpers
from aif360.metrics import utils

DEFAULT = 'Testing'

class FairPredictor:
	def __init__(self,unprivileged_groups,privileged_groups):
		self.metric = None
		self.unprivileged_groups = unprivileged_groups
		self.privileged_groups = privileged_groups

	def fit_and_transform(self, data):
		#self.describe_metrics(data)
		train, test = data.split([0.8], shuffle=True)
		print("Before : \n")
		self.describe_metrics(train)

		options = [DisparateImpactRemover(repair_level=0),
				Reweighing(unprivileged_groups=self.unprivileged_groups,privileged_groups=self.privileged_groups), 
				#LFR(unprivileged_groups=self.unprivileged_groups,privileged_groups=self.privileged_groups), 
				DisparateImpactRemover(repair_level=0.25),
				DisparateImpactRemover(repair_level=0.5),
				DisparateImpactRemover(repair_level=0.75),
				DisparateImpactRemover(repair_level=1)]
		
		best_accuracy = float('-inf')
		best_model = None
		best_trans_data = None 

		for option in options:
			metric = option
			trans_train = metric.fit_transform(train)
			#Standard Scaler??
			X_train = trans_train.features
			y_train = trans_train.labels
			X_test = test.features
			y_test = test.labels
			current_model = tree.DecisionTreeClassifier().fit(X_train,y_train)
			predicted = current_model.predict(X_test)
			#accuracy
			acc = current_model.score(X_test,y_test)
			#disparate impact score
			test_for_disparate = test.copy()
			test_for_disparate.labels = predicted
			metrics = BinaryLabelDatasetMetric(test_for_disparate,unprivileged_groups=self.unprivileged_groups,privileged_groups=self.privileged_groups)
			disp_imp = metrics.disparate_impact()

			print(option, disp_imp, acc)

			if disp_imp >= 0.8 and acc > best_accuracy:
				best_trans_data = trans_train
				best_model = current_model
				best_accuracy = acc

		if best_accuracy == float('-inf'):
			print("RAISE EXCEPTION")
			return tree.DecisionTreeClassifier().fit(train.features, train.labels)
		else:
			print("After: \n")
			self.describe_metrics(best_trans_data)
			return best_model, best_accuracy 

	def describe_metrics(self, dataset):
		metric_arrs = defaultdict(list)
		metric = BinaryLabelDatasetMetric(dataset, 
			unprivileged_groups=self.unprivileged_groups,
			privileged_groups=self.privileged_groups)
		# metric_arrs['bal_acc'].append((metric.num_positives() + metric.num_negatives()) / 2)
		metric_arrs['disp_imp'].append(metric.disparate_impact())
		metric_arrs['stat_par_diff'].append(metric.statistical_parity_difference())
		print("Disparite Impact Metric: {0}".format(metric_arrs["disp_imp"][0]))
		print("Statistical Parity Difference Metric: {0}".format(metric_arrs["stat_par_diff"][0]))
		# metric_arrs['consistency'].append(metric.consistency())
		# best_ind = np.argmax(metric_arrs['bal_acc'])
		# print("Best balanced accuracy: {:6.4f}".format(metric_arrs['bal_acc'][best_ind]))
		#  disp_imp_at_best_ind = np.abs(1 - np.array(metrics['disp_imp']))[best_ind]
		# disp_imp_at_best_ind = 1 - min(metric_arrs['disp_imp'][best_ind], 1/metric_arrs['disp_imp'][best_ind])
		# print("Corresponding 1-min(DI, 1/DI) value: {:6.4f}".format(disp_imp_at_best_ind))
		# print("Corresponding statistical parity difference value (mean difference): {:6.4f}"\
		# 	.format(metric_arrs['stat_par_diff'][best_ind]))





